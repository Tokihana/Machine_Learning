# h5py

h5py是一种常见的存储大数据集的格式，这种格式能够有效压缩数据的大小

创建h5数据

```py
import h5py
with h5py.File(filename, 'w') as file:
    file.create_dataset(dataset_name=, shape=, dtype=)
```

分批读取h5数据

```py
with h5py.File(filename, 'r') as file: # 读取最好用'r'，避免意外修改文件
    data = h5py[dataset_name] # 这一步创建的是索引，不会真正读入数据
    for index in range(0, data.shape[1], batch_size):
        chunk = data[index, index+batch_size] # 这一步真正读入了数据
```



# tensorflow的分batch读取

tensorflow支持分batch读取数据，不过对于不能一口气加载进内存的数据，最好的方法还是尝试构建输入流水线（input pipeline）

```py
import tensorflow as tf
```

一种方法是使用`from_tensors()`或者`from_tensor_slices()`，数据需要**能够读入内存**

```py
# 这种方式适合能够读入内存的数据
dataset = tf.data.Dataset.from_tensor_slices([8, 3, 0])
```

`Dataset`是可迭代的，可以用for循环遍历，也可以显式构造迭代器迭代

```py
# 使用for循环的迭代方式
for elem in dataset:
	print(elem.numpy())
# 使用迭代器的方式
it = iter(dataset)
print(next(it).numpy())
```



对于不可读入内存的数据，可以使用python生成器

```py
# 通常的表示形式，yield用于返回一行数据
def data_generator():
    while True:
        yield i
```

使用生成器创建`tf.data.Dataset`，必须传入`output_types`

```py
dataset = tf.data.Dataset.from_generator(data_generator, # 生成器
                                         args=[], # 生成器的参数列表，如果有的话
                                         output_signature=(
                                         	tf.TensorSpec(shape=(), dtype=tf.float64,
                                                         ),
                                         ) # 数据大小
```

通过调用`.batch`方法，可以将`Dataset`按batch划分



还有一种方式可以直接从csv中按需要加载

```py
batchs = tf.data.experimental.make_csv_dataset(file, 
                                              batch_size,
                                              label_name = )
for fea_batch, label_batch in batchs.take(1):
    print(label_batch)
    for key, value in fea_batch:
        print(key, values)
```



# 构建适用于h5文件的sequence类

[参考](https://saturncloud.io/blog/keras-custom-data-generator-for-large-hdf5-files-a-comprehensive-guide/)

[手册](https://www.tensorflow.org/api_docs/python/tf/keras/utils/Sequence)

导入库

```py
import numpy as np
import h5py
import tensorflow as tf
from tensorflow.keras.utils import Sequence # 这个对象用于拟合数据序列，例如dataset
										# 适合并行处理，该数据结构保证神经网络每epoch只在每个样例上训练一次
    									# generator不能保证这一点
```

创建`HDF5DataGenefator`类

```py
class HDF5DataGenerator(Sequence):
    def __init__(self, hdf5_file, batch_size, shuffle=True):
        self.hdf5_file = h5py.File(file_name, 'r')
        self.batch_size = batch_size
        self.shuffle = shuffle
        self.indexs = np.arange(len(self.hdf5_file['data']))
        if self.shuffle:
            np.random.shuffle(self.indexes) # 这里打乱index而不是打乱data，应该是因为h5要求index必须升序的原因
```

因为keras需要知道数据集的大小，所以定义`__len__`方法

```py
def __len__(self):
	return int(np.ceil(len(self.hdf5_file['data']) / float(self.batch_size)))
```

`__getitem__`方法检索对应index的batch，用于从hdf5文件加载和返回batch

```py
def __getitem__(self, index):
    indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]
    batch_data = np.array(self.hdf5_file['data'][indexs])
    batch_labels = np.array(self.hdf5_file['labels'][indexs])
    return batch_data, batch_labels
```

定义`on_epoch_end`方法，如果`shuffle=True`，则在每轮结束后重新打乱index

```py
def on_epoch_end(self):
	if self.shuffle:
        np.random.shuffle(self.indexes)
```

> `__getitem__`和`__len__`方法是必须实现的，`on_epoch_end`方法在每轮结束后调用
>
> `__getitem__`方法返回整个batch

使用sequence类进行训练

```py
model = Sequential()
model.compile()
generator = HDF5DataGenerator
model.fit(generator, epochs = 10)
```

