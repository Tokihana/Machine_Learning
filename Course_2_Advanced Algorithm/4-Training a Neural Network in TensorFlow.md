# TensorFlow implements

ä½¿ç”¨æ­¤å‰è®¨è®ºè¿‡çš„é¢„æµ‹æ‰‹å†™ä½“0/1çš„ä¾‹å­ï¼Œé¦–å…ˆç»™å‡ºè®­ç»ƒç½‘ç»œçš„ä»£ç ï¼Œç„¶ååˆ†åˆ«åˆ†ææ¯ä¸ªéƒ¨åˆ†åœ°ä½œç”¨

![image-20230717084748341](D:\CS\Machine Learning\Course_2_Advanced Algorithm\4-Training a Neural Network in TensorFlow.assets\image-20230717084748341.png)

```py
import tensorflow as tf
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense
# 1: sequence model
model = Sequential([
	Dense(units = 25, activation = 'sigmoid'),
	Dense(units = 15, activation = 'sigmoid'),
	Dense(units = 1, activation = 'sigmoid')
])
# 2: compile model
from tensorflow.keras.losses import BinaryCrossentropy
model.compile(loss = BinaryCrossentropy())
# 3: training the model
model.fit(X, Y, epochs = 100)
```



å›å¿†ä¸€ä¸‹æ­¤å‰è®­ç»ƒæ¨¡å‹çš„æ­¥éª¤ï¼Œå¯ä»¥åˆ’åˆ†ä¸ºä¸‹é¢å‡ æ­¥ï¼š

1. ç¡®å®šå¦‚ä½•æ ¹æ®è¾“å…¥`x`è®¡ç®—è¾“å‡ºã€‚åœ¨ç¥ç»ç½‘ç»œä¸­ï¼Œè¿™ä¸€æ­¥å¯¹åº”inferenceã€‚
2. ç¡®å®šlosså’Œcostï¼Œlossæ˜¯å•ä¸ªæ ·ä¾‹çš„å·®å¼‚ï¼Œcostæ˜¯æ•´ä¸ªæ ·æœ¬çš„å·®å¼‚ã€‚ä¸Šé¢çš„ä»£ç ä½¿ç”¨äº†binary cross entropyä½œä¸ºlossï¼›å¦‚æœéœ€è¦è®­ç»ƒlinear regressionï¼Œå¯ä»¥ä½¿ç”¨`MeanSquareError()`
3. è®­ç»ƒæ¨¡å‹ï¼Œæœ€å°åŒ–costã€‚æˆ‘ä»¬æ›¾åœ¨çº¿æ€§å›å½’å’Œå¯¹ç‡å›å½’è®¨è®ºäº†å¦‚ä½•ä½¿ç”¨æ¢¯åº¦ä¸‹é™åšåˆ°è¿™ä¸€ç‚¹ã€‚TensorFlowåœ¨è°ƒç”¨`.fit`çš„æ—¶å€™ä¼šè¿›è¡Œåå‘ä¼ æ’­ï¼ˆback propagationï¼‰è®¡ç®—æ¯éƒ¨åˆ†çš„å¯¼æ•°é¡¹ã€‚



# Entropy & Binary Cross Entropy

åœ¨æ­¤å‰å…³äºlogistic regressionçš„è®¨è®ºä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†ä¸‹é¢çš„losså‡½æ•°ï¼š
$$
Loss(f_{\vec w, b}(\vec x_i), y_i) = -y_i \log(f_{\vec w, b}(\vec x_i)) - (1 - y_i) \log(1 - f_{\vec w, b}(\vec x_i))
$$
åœ¨ç»Ÿè®¡å­¦ä¸Šï¼Œè¿™ä¸ªå‡½æ•°è¢«ç§°ä¸ºbinary cross entropyï¼ŒäºŒåˆ†ç±»äº¤å‰ç†µï¼›TensorFlowåŒæ ·é‡‡ç”¨äº†è¿™ä¸ªæœ¯è¯­ï¼Œä¹Ÿå³ä»£ç ä¸­çš„`BinaryCrossentropy()`ã€‚

> è®¨è®ºlogistic regressionçš„æ—¶å€™ï¼Œè¿™ä¸ªlossæ˜¯ç”¨MLEï¼ˆæå¤§ä¼¼ç„¶æ³•ï¼‰æ¨å¯¼çš„ï¼Œè¿™é‡Œè®¨è®ºä¸‹äº¤å‰ç†µæ³•ï¼ˆCross entropy, CEï¼‰ã€‚



## Entropy

åœ¨ä¿¡æ¯è®ºä¸­ï¼Œç†µï¼ˆentropyï¼‰çš„å«ä¹‰æ˜¯**æ— æŸç¼–ç äº‹ä»¶çš„æœ€å°å¹³å‡ç¼–ç é•¿åº¦**ï¼Œæˆ‘ä»¬æ›¾ç»åœ¨æ•°æ®ç»“æ„ä¸­å­¦ä¹ è¿‡éœå¤«æ›¼ç¼–ç ï¼Œé€šè¿‡å¯¹å‡ºç°é¢‘ç‡é«˜çš„å­—ç¬¦ä½¿ç”¨çŸ­ç¼–ç ï¼Œå¯¹å‡ºç°é¢‘ç‡ä½çš„å­—ç¬¦ä½¿ç”¨é•¿ç¼–ç ï¼Œéœå¤«æ›¼ç¼–ç å¯ä»¥è®¾è®¡å‡ºé•¿åº¦æœ€çŸ­çš„äºŒè¿›åˆ¶å‰ç¼€ç ï¼Œ$\frac {å­—ç¬¦ä¸²ç é•¿}{å­—ç¬¦æ•°}$å°±æ˜¯æœ€å°å¹³å‡ç¼–ç é•¿åº¦ã€‚æˆ‘ä»¬å¯ä»¥å‡å®šå­—ç¬¦ï¼ˆäº‹ä»¶ï¼‰çš„å‡ºç°æœä»æŸç§æ¦‚ç‡åˆ†å¸ƒï¼Œåœ¨æŸä¸€æ ·æœ¬ä¸­ï¼Œäº‹ä»¶å‡ºç°çš„æ¦‚ç‡å¯ä»¥è¡¨è¿°ä¸ºï¼š
$$
P = \frac {è¯¥äº‹ä»¶å‘ç”Ÿçš„æ¬¡æ•°} {æ€»è¯•éªŒæ•°}
$$


é¦–å…ˆè®¨è®ºä¸€ç§ç®€å•çš„æƒ…å†µï¼Œè®¾æœ‰Nç§ç­‰å¯èƒ½çš„äº‹ä»¶ï¼Œåˆ™æ¯ç§äº‹ä»¶å‘ç”Ÿçš„æ¦‚ç‡ä¸º$P = \frac 1 N$ï¼Œæ­¤æ—¶ç¼–ç è¯¥ä¿¡æ¯çš„æœ€å°é•¿åº¦å¯ä»¥è¡¨è¿°ä¸º
$$
\log_2N = -\log_2\frac 1 N = -\log_2P = -\sum P\log_2P
$$

> åº•æ•°å–2æ˜¯å› ä¸ºè¦è¿›è¡ŒäºŒè¿›åˆ¶ç¼–ç 



æ¨å¹¿åˆ°å¯èƒ½æ€§ä¸å‡ä¸€çš„æƒ…å†µï¼Œä»¥åŠè¿ç»­å˜é‡çš„ç†µï¼ˆå•ä½ä¸ºbitsï¼‰
$$
Entropy = -\sum_i P(i) \log_2P(i)
\newline
Entropy = -\int P(x) \log_2P(x) dx
$$

> ä»ç›´è§‚ä¸Šç†è§£ï¼Œç†µåæ˜ äº†ä¿¡æ¯ä¸­å¯èƒ½çŠ¶æ€çš„å¤šå°‘ã€‚è‹¥ç†µæ¯”è¾ƒå¤§ï¼Œåˆ™éšæœºå˜é‡çš„ä¸ç¡®å®šæ€§è¾ƒå¼ºï¼Œéš¾ä»¥å¯¹æ–°ä¿¡æ¯è¿›è¡Œé¢„æµ‹ã€‚
>
> ä»¥äºŒé¡¹åˆ†å¸ƒä¸ºä¾‹ï¼Œè®¾$P(x = 1) = p,\ P(x = 0) = 1-p$ï¼Œåˆ™ç†µä¸º
> $$
> H(p) = -p \log_2 p - (1-p)\log_2(1-p)
> $$
> å…¶æ›²çº¿ä¸º
>
> ![image-20230717103405092](D:\CS\Machine Learning\Course_2_Advanced Algorithm\4-Training a Neural Network in TensorFlow.assets\image-20230717103405092.png)
>
> å¯ä»¥è§‚å¯Ÿåˆ°ï¼Œå½“ç†µæœ€å¤§çš„æ—¶å€™ï¼Œp = 0.5ï¼Œéšæœºå˜é‡çš„ä¸ç¡®å®šæ€§æœ€å¤§ã€‚



ç†µå…¬å¼åŒæ ·å¯ä»¥ç†è§£ä¸ºï¼Œå¯¹ä¸€ä¸ªéšæœºå˜é‡ï¼Œè®¡ç®—å…¶$-\log P(x)$çš„æœŸæœ›ï¼Œå› æ­¤ç†µå…¬å¼åˆå¯ä»¥ç®€å†™ä¸ºï¼š
$$
H(P) = Entropy = \mathrm{E}_{x\sim P}(-\log P(x))
$$
$x\sim P$ä»£è¡¨ä½¿ç”¨Pè®¡ç®—æœŸæœ›ï¼Œä»å¼ä¸­å¯çŸ¥ï¼Œç†µåªä¾èµ–äºåˆ†å¸ƒï¼Œè€Œä¸xçš„å–å€¼æ— å…³ã€‚



## Cross-Entropy

å‰é¢è®¨è®ºåˆ°ï¼Œå¦‚æœæˆ‘ä»¬å·²çŸ¥æŸäº‹ä»¶æœä»æ¦‚ç‡åˆ†å¸ƒPï¼Œå°±å¯ä»¥è®¡ç®—è¯¥äº‹ä»¶çš„ç†µã€‚ç„¶è€Œï¼Œåœ¨å®é™…åœºæ™¯ä¸­ï¼Œé€šå¸¸æ— æ³•å¾—çŸ¥çœŸå®åˆ†å¸ƒPï¼Œæ­¤æ—¶å°±éœ€è¦å¯¹ç†µåšä¸€ä¸ªä¼°è®¡ã€‚

é¦–å…ˆå¯¹ç†µå…¬å¼ä¸­çš„ä¸åŒéƒ¨åˆ†è¿›è¡Œåˆ’åˆ†ï¼Œåœ¨å…¬å¼
$$
Entropy = \mathrm{E}_{x\sim P}(-\log P(x))
$$
ä¸­ï¼Œ$-\log P(x)$æŒ‡å®šäº†ç¼–ç çš„æ–¹å¼ï¼Œè€Œ$E_{x \sim }$åˆ™æŒ‡å®šäº†è¢«ç¼–ç çš„å˜é‡çš„åˆ†å¸ƒã€‚



è®¾åœ¨è§‚æµ‹ä¹‹å‰ï¼Œé¢„ä¼°çš„æ¦‚ç‡åˆ†å¸ƒä¸ºQï¼Œè§‚æµ‹åå¾—åˆ°çš„æ¦‚ç‡åˆ†å¸ƒä¸ºPï¼Œåˆ™äº¤å‰ç†µ
$$
Cross\ Entropy = \mathrm{E}_{x \sim P}(-\log Q(x))
$$
è®°$H(P, Q)$ï¼Œè¡¨ç¤º**ä½¿ç”¨åŸºäºQçš„ç¼–ç ï¼Œå¯¹æ¥è‡ªPçš„å˜é‡ç¼–ç æ‰€éœ€è¦çš„å­—èŠ‚æ•°**ã€‚è¯¥å€¼å¯ä»¥ç”¨äºè¡¡é‡é¢„ä¼°åˆ†å¸ƒQæ˜¯å¦èƒ½å¤Ÿååº”çœŸå®çš„åˆ†å¸ƒPã€‚

æ ¹æ®ç†µå’Œäº¤å‰ç†µçš„å®šä¹‰ï¼Œæ˜“çŸ¥$H(P, Q) \ge H(P)$ï¼Œå› ä¸º$H(P)$æ˜¯ç†è®ºæœ€å°å€¼ï¼Œæ‰€ä»¥$H(P, Q)$åªå¯èƒ½å¤§äºç­‰äº$H(P)$ã€‚è€Œäº¤å‰ç†µè¶Šæ¥è¿‘ç†è®ºæœ€å°å€¼ï¼Œé¢„ä¼°åˆ†å¸ƒQå°±è¶Šæ¥è¿‘çœŸå®åˆ†å¸ƒPï¼Œè¿™ä¹Ÿæ˜¯ä¸ºä»€ä¹ˆå¯ä»¥ä½¿ç”¨äº¤å‰ç†µä½œä¸ºæŸå¤±å‡½æ•°çš„åŸå› ã€‚



åœ¨äºŒåˆ†ç±»ä»»åŠ¡ä¸­ï¼ˆæˆ–è€…è¯´Pæœä»äºŒé¡¹åˆ†å¸ƒçš„æƒ…å†µä¸‹ï¼‰ï¼Œæœ‰
$$
H(P, Q) = -P \log_2 Q - (1-P)\log_2(1-Q)
$$
ç§°ä¸ºbinary cross entropyã€‚

> æˆ‘ä»¬æ­¤å‰è®¨è®ºè¿‡ï¼Œlogistic regressionçš„losså¯ä»¥å†™ä¸º
> $$
> loss = -y\log f(x) - (1-y) \log (1 - f(x))
> $$
> è€Œcoståˆ™æ˜¯å–æ‰€æœ‰lossçš„å‡å€¼ï¼Œç­‰ä»·äºè¿™é‡Œçš„äº¤å‰ç†µã€‚

# Alternatives to the sigmoid activation

åœ¨æ­¤å‰çš„ç¥ç»ç½‘ç»œåˆ†ç±»ä»»åŠ¡ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨çš„æ¿€æ´»å‡½æ•°éƒ½æ˜¯`sigmoid`ï¼Œè¯¥å‡½æ•°å‡å®šè¾“å‡ºç»“æœéƒ½æ˜¯äºŒå…ƒçš„ã€‚ä¾‹å¦‚åœ¨é¢„æµ‹äº§å“æ˜¯å¦ä¼šç•…é”€çš„ä¾‹å­ä¸­ï¼Œæˆ‘ä»¬å‡è®¾éšå±‚å±æ€§ä¸º`[affordability, awareness, perceived quality]`ï¼Œåˆ™è¯¥å±‚çš„è¾“å‡ºä¸ºæ¶ˆè´¹è€…æ˜¯å¦æœ‰èƒ½åŠ›è´­ä¹°ã€æ˜¯å¦å·²ç»è¢«æ¶ˆè´¹è€…äº†è§£ã€ä»¥åŠæ¶ˆè´¹è€…æ˜¯å¦è®¤ä¸ºäº§å“è´¨é‡å¥½ï¼›è¿™äº›éƒ½æ˜¯äºŒå…ƒçš„å±æ€§ã€‚

è€Œåœ¨å®é™…çš„åº”ç”¨åœºæ™¯ä¸­ï¼Œè¿™äº›å±æ€§å¯èƒ½å¹¶éäºŒå…ƒçš„ï¼Œå–å€¼èŒƒå›´ä¹Ÿå¯èƒ½å¹¶é`[0,1]`ï¼Œé€‰æ‹©ä¸åŒçš„æ¿€æ´»å‡½æ•°ï¼Œèƒ½å¤Ÿæ”¹å–„æ¨¡å‹çš„æ€§èƒ½ã€‚



Sigmoidä¹‹å¤–ï¼Œä¸€ç§å¸¸è§çš„æ¿€æ´»å‡½æ•°æ˜¯ReLUï¼ˆRectified linear unitï¼‰ï¼Œè¯¥å‡½æ•°çš„æ•°å­¦è¡¨ç¤ºä¸º$g(z) = max(0,z)$ï¼Œå›¾åƒä¸º

![image-20230717114522336](D:\CS\Machine Learning\Course_2_Advanced Algorithm\4-Training a Neural Network in TensorFlow.assets\image-20230717114522336.png)



åœ¨å‰é¢çš„è®¨è®ºä¸­ï¼Œä¹Ÿä½¿ç”¨è¿‡linearï¼Œæœ‰æ—¶ä¼šä¼šæœ‰äººè¯´â€œæ²¡æœ‰ä½¿ç”¨æ¿€æ´»å‡½æ•°â€ï¼Œä¹Ÿæ˜¯æŒ‡çš„ç”¨linear

![image-20230717114900790](D:\CS\Machine Learning\Course_2_Advanced Algorithm\4-Training a Neural Network in TensorFlow.assets\image-20230717114900790.png)



# Choosing activation functions

## Output layer

è¾“å‡ºå±‚çš„æ¿€æ´»å‡½æ•°å¯ä»¥æ ¹æ®è¾“å‡ºçš„targetå†³å®šã€‚ä¾‹å¦‚

- äºŒåˆ†ç±»é—®é¢˜ï¼Œ$y = 0/1$ï¼Œä½¿ç”¨sigmoid
- å›å½’é—®é¢˜ï¼Œyå¯ä»¥æ˜¯æ­£å€¼æˆ–è´Ÿå€¼ï¼Œä½¿ç”¨linear
- å›å½’é—®é¢˜ï¼Œyå¿…é¡»æ˜¯æ­£å€¼ï¼ˆä¾‹å¦‚æˆ¿ä»·ï¼‰ï¼Œä½¿ç”¨ReLU



## Hidden layer

**ReLU**æ˜¯å½“ä¸‹çš„å¸¸è§é€‰æ‹©ï¼Œæœ€æ—©æœŸå¾ˆå¤šäººç”¨sigmoidã€‚

åŸå› æœ‰å¾ˆå¤šï¼Œé¦–å…ˆReLUçš„æ•°å­¦è®¡ç®—å¾ˆå¿«ï¼Œ$g(z) = max(0, z)$æ¯”sigmoidçš„$\frac 1 {1 + e^{-z}}$å¿«å¾—å¤šï¼›å…¶æ¬¡ï¼Œä»ReLUçš„å‡½æ•°å›¾åƒæ¥çœ‹ï¼ŒReLUåªä¼šåœ¨å·¦åŠçš„éƒ¨åˆ†æ‰å¹³ï¼Œè¿™ä½¿å¾—å¯¼æ•°å€¼æ›´å¤§ï¼ˆå› ä¸ºå¯¼æ•°ååº”æ–œç‡ï¼‰ï¼Œæ¢¯åº¦ä¸‹é™é€Ÿåº¦æ›´å¿«ã€‚

> ReLUè¿˜æœ‰ä¸ªä¼˜åŠ¿æ˜¯ç¨€ç–æ¿€æ´»ï¼ˆsparsely activatedï¼‰ï¼Œå› ä¸ºå¯¹æ‰€æœ‰çš„è´Ÿå€¼ç»“æœï¼Œè¾“å‡ºéƒ½ä¸º0ã€‚ä»è€Œå¯ä»¥é¿å…è®¡ç®—æ— å…³éƒ¨åˆ†ã€‚



æœ‰æ—¶æ–‡çŒ®é‡Œè¿˜èƒ½çœ‹åˆ°å…¶ä»–çš„æ¿€æ´»å‡½æ•°ï¼Œä¾‹å¦‚`tanh`, `LeckyReLU`ç­‰ï¼Œåœ¨æŸäº›æƒ…å†µä¸‹ï¼Œè¿™äº›æ¿€æ´»å‡½æ•°æ•ˆæœæ›´å¥½ï¼Œä¸è¿‡å¯¹å¤§å¤šæ•°ç¥ç»ç½‘ç»œæ¥è¯´ï¼Œä½¿ç”¨ReLUå°±å¤Ÿç”¨äº†ã€‚

> LeckyReLUå’ŒReLUä¸åŒçš„åœ°æ–¹åœ¨äºè´Ÿå€¼åŒºï¼ŒReLUçš„è´Ÿå€¼åŒºç›´æ¥å½’0äº†ï¼Œè€ŒLeckyReLUè¿˜ä¿æŒä¸€ä¸ªéå¸¸å°çš„æ–œç‡ï¼Œè¿™æ ·å¯ä»¥é¿å…æŸäº›æƒ…å†µè¥¿ReLUå½’0å¯¼è‡´ç¥ç»å…ƒâ€æ­»äº¡â€œã€‚



# Why do we need activation functions

å‡è®¾ä¸ä½¿ç”¨æ¿€æ´»å‡½æ•°ï¼ˆå…¨ç”¨linearï¼‰ï¼Œç¥ç»ç½‘ç»œçš„ç»“æœå’Œçº¿æ€§å›å½’ä¸ä¼šæœ‰ä»€ä¹ˆä¸åŒã€‚

ç”¨ä¸€ä¸ªç®€å•çš„æ¨¡å‹æ¥è¯´æ˜ï¼Œåœ¨ä¸‹é¢è¿™ä¸ªæ¨¡å‹ä¸­ï¼Œæ‰€æœ‰èŠ‚ç‚¹çš„æ¿€æ´»å‡½æ•°éƒ½æ˜¯linear

![image-20230717163544135](D:\CS\Machine Learning\Course_2_Advanced Algorithm\4-Training a Neural Network in TensorFlow.assets\image-20230717163544135.png)

æ­¤æ—¶æœ‰
$$
a^{[1]} = w_1^{[1]}x + b_1^{[1]}
\newline
a^{[2]} = w_1^{[2]} a^{[1]} + b_1^{[2]}
$$
å°†$a^{[1]}$å¸¦å…¥ç¬¬äºŒä¸ªå¼å­ï¼Œåˆ™
$$
a^{[2]} = w_1^{[2]} (w_1^{[1]}x + b_1^{[1]}) + b_1^{[2]} 
\newline
= w_1^{[2]}w_1^{[1]}x + w_1^{[2]}b_1^{[1]} + b_1^{[2]} 
$$
å®è´¨ä¸Šç­‰ä»·äºä¸€ä¸ªæ–°çš„çº¿æ€§å‡½æ•°ã€‚ä»çº¿æ€§ä»£æ•°çš„è§’åº¦ä¸Šæ¥çœ‹ï¼Œç›¸å½“äºå¯¹çº¿æ€§å˜æ¢è¿›è¡Œç»„åˆï¼Œç»“æœè¿˜æ˜¯ä¸€ä¸ªçº¿æ€§å˜æ¢ã€‚

å› æ­¤ï¼Œå¯¹äºå¤šå±‚çš„ç¥ç»ç½‘ç»œï¼Œéšå±‚ä¸è¦ä½¿ç”¨linearæ¿€æ´»å‡½æ•°ï¼Œè¿™æ ·åšåªæ˜¯å¯¹ç»“æœåšäº†ä¸ªçº¿æ€§å˜æ¢ï¼Œç­‰ä»·äºä»€ä¹ˆéƒ½æ²¡åšã€‚



# Back Propagation

Back Propagationï¼ˆåå‘ä¼ æ’­ï¼Œè¯¯å·®é€†ä¼ æ’­ï¼Œç®€å†™BPï¼‰æ˜¯æœ€å¸¸ç”¨çš„ç¥ç»ç½‘ç»œå­¦ä¹ ç®—æ³•ã€‚è¿™é‡Œè®¨è®ºä¸‹å¦‚ä½•ç›´è§‚ç†è§£åå‘ä¼ æ’­ã€‚



## Derivative

é¦–å…ˆä¸¾ä¸€ä¸ªéå¸¸ç®€å•çš„ä¾‹å­æ¥ç›´è§‚ç†è§£å¯¼æ•°ï¼Œè®¾costå‡½æ•°
$$
J(w) = w^2
$$
è‹¥$w = 3$ï¼Œåˆ™$J(w) = 9$ï¼Œå‡å®š$w$å˜åŒ–ä¸€ä¸ªæå°å€¼ï¼Œ$\varepsilon = 0.001/0.002$ï¼Œåˆ™
$$
\Delta w = 0.001/0.002\newline
\Delta J(w) = 0.006001/0.012004
$$
å¯ä»¥è§‚å¯Ÿåˆ°$\Delta J(w) \approx 6 \Delta w$ï¼Œéšç€$\varepsilon$çš„å‡å°ï¼Œè¿™ä¸ªç»“æœä¼šæ›´åŠ ç²¾ç¡®ã€‚

æ ¹æ®å·²æœ‰å¾®ç§¯åˆ†çŸ¥è¯†ä¹Ÿå¯ä»¥æ¨å¾—ï¼Œ$\frac d {dw} w^2 = 2w$ï¼Œå› ä¸º$w = 3$ï¼Œæ‰€ä»¥å¯¼æ•°ä¸º6ã€‚å¯¼æ•°åæ˜ äº†$J(w)$éš$w$çš„å˜åŒ–çš„æ¯”ç‡ã€‚



åœ¨è¿›è¡Œæ¢¯åº¦ä¸‹é™çš„æ—¶å€™ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸‹é¢çš„æ–¹æ³•æ›´æ–°$w_j$
$$
w_j = w_j - \alpha \frac {\partial} {\partial w_j}J(\vec w, b)
$$
å½“å¯¼æ•°å¾ˆå¤§çš„æ—¶å€™ï¼Œæ„å‘³ç€$w$çš„å¾®å°å˜åŒ–éƒ½ä¼šå¯¹$J$çš„äº§ç”Ÿå¾ˆå¤§çš„å½±å“ï¼Œè¿™ç§æ—¶å€™å°±éœ€è¦è°ƒæ•´$w$ï¼Œä½¿å…¶ä¸ä¼šå¯¹$J$äº§ç”Ÿå¾ˆå¤§å½±å“ï¼›åä¹‹ï¼Œè‹¥å¯¼æ•°å¾ˆå°ï¼Œåˆ™$w$å¯¹$J$å½±å“ä¸å¤§ï¼Œä¹Ÿä¸éœ€è¦å¯¹$w$åšå‡ºå¤ªå¤šçš„æ”¹å˜ã€‚



ä»å‡½æ•°å›¾åƒä¸Šæ¥çœ‹ï¼Œå¯¼æ•°åæ˜ äº†æ›²çº¿çš„æ–œç‡

![image-20230719160408259](D:\CS\Machine Learning\Course_2_Advanced Algorithm\4-Training a Neural Network in TensorFlow.assets\image-20230719160408259.png)



## SymPy

SymPyæ˜¯ä¸€ä¸ªç¬¦å·æ•°å­¦åº“ï¼ˆsymbolic mathematical libraryï¼‰ï¼Œç¬¦å·è®¡ç®—çš„æ„æ€æ˜¯ï¼Œåœ¨é»˜è®¤æƒ…å†µä¸‹ä¸ä¼šè®¡ç®—éç²¾ç¡®æ•°å€¼ï¼Œè€Œæ˜¯ä¼šä¼ é€’symbolå¯¹è±¡ï¼Œè¿™æ ·ä¸€æ–¹é¢å¯ä»¥é˜²æ­¢ç²¾åº¦æŸå¤±ï¼Œå¦ä¸€æ–¹é¢å¯ä»¥ç®€åŒ–æŸäº›è¿ç®—ã€‚

ä¸¾ä¾‹æ¥è¯´ï¼Œè®¡ç®—$\sqrt 8$

```py
>>> import math
>>> import sympy
>>> math.sqrt(8)
2.82842712475
>>> sympy.sqrt(8)
2*sqrt(2)
```



ä½¿ç”¨SymPyè®¡ç®—å¯¼æ•°

```py
>>> import sympy
# è®¾ç½®symbolå˜é‡
>>> J, w = sympy.symbols('J, w')
>>> J = w**2
>>> J
ğ‘¤2
# æ±‚å¯¼æ•°
>>> dJ_dw = sympy.diff(J, w)
>>> dJ_dw
2ğ‘¤
# å¸¦å…¥å€¼è®¡ç®—å¯¼æ•°
>>> dJ_dw.subs([(w, 3)])
6
```



## Computation Graph & Prop.

ç¥ç»ç½‘ç»œæ˜¯ä¸€ç§ç‰¹æ®Šå½¢å¼çš„è®¡ç®—å›¾ã€‚è®¡ç®—å›¾æ˜¯æœ‰å‘å›¾ï¼Œå…¶èŠ‚ç‚¹å¯¹åº”**å˜é‡ï¼ˆvariablesï¼‰**æˆ–è€…**æ“ä½œï¼ˆoperationsï¼‰**ï¼Œå˜é‡çš„å€¼è¢«ä¼ å…¥æ“ä½œï¼Œæ“ä½œçš„è¾“å‡ºå€¼å¯ä»¥ä¼ å…¥æ–°çš„æ“ä½œã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œè®¡ç®—å›¾å¯ä»¥è¡¨ç¤ºç‰¹å®šçš„è¿ç®—ï¼Œä¾‹å¦‚ä¸‹é¢çš„è®¡ç®—å›¾å°±è¡¨ç¤ºäº†ä»¿å°„å˜æ¢ï¼ˆaffine transformationï¼‰

![image-20230719165812988](D:\CS\Machine Learning\Course_2_Advanced Algorithm\4-Training a Neural Network in TensorFlow.assets\image-20230719165812988.png)



é¦–å…ˆä»ä¸€ä¸ªç®€å•çš„çº¿æ€§å›å½’ä¾‹å­å¼€å§‹ï¼Œå­¦ä¹ å¦‚ä½•å°†è¿ç®—å±•å¼€ä¸ºè®¡ç®—å›¾ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œå¯¹å•ä¸ªçº¿æ€§ç¥ç»å…ƒçš„ç¥ç»ç½‘ç»œï¼Œå…¶costå‡½æ•°ä¸º$J(w, b) = \frac 1 2 (wx + b - y)^2$ï¼Œå°†æ¯ä¸€æ­¥è¿ç®—å±•å¼€

![image-20230719170815116](D:\CS\Machine Learning\Course_2_Advanced Algorithm\4-Training a Neural Network in TensorFlow.assets\image-20230719170815116.png)

![image-20230719171109111](D:\CS\Machine Learning\Course_2_Advanced Algorithm\4-Training a Neural Network in TensorFlow.assets\image-20230719171109111.png)

è¿™å¼ è®¡ç®—å›¾å¯¹åº”ç€ç¥ç»ç½‘ç»œçš„å‰å‘ä¼ æ’­ï¼ˆforward prop.ï¼‰ï¼Œè¿›ä¸€æ­¥ï¼Œæˆ‘ä»¬å¯ä»¥åˆ©ç”¨è¿™å¼ è®¡ç®—å›¾ï¼Œé€šè¿‡åå‘ä¼ æ’­ï¼ˆback prop.ï¼‰æ¨å¯¼å¯¼æ•°



é¦–å…ˆè®¡ç®—$d$ä¸$J$çš„å…³ç³»ï¼Œå› ä¸º
$$
\frac {\partial J}{\partial d} = d
$$
å› æ­¤ï¼Œå¦‚æœ$d = d + \varepsilon$ï¼Œåˆ™$J = J + d * \varepsilon$ã€‚åœ¨ä¸Šå›¾ä¸­ï¼Œå› ä¸º$d = 2$ï¼Œå› æ­¤å¯¼æ•°å€¼ä¸º2



è¿›ä¸€æ­¥æ¨ç®—$\frac {\partial J}{\partial a}$
$$
\begin{array}{l}
\frac {\partial d}{\partial a} = 1
\newline\to
\frac {\partial J}{\partial a} 
= \frac {\partial J}{\partial d} \frac {\partial d}{\partial a}
= d = 2
\end{array}
$$


åŒç†ï¼Œæ¨å¯¼$\frac {\partial J}{\partial b}, \frac {\partial J}{\partial c}$
$$
\begin{array}{l}
\frac {\partial a}{\partial b} = 1
\newline
\to \frac {\partial J}{\partial b} 
= \frac {\partial J}{\partial d} \frac {\partial d}{\partial a} \frac {\partial a}{\partial b} 
= b = 2
\newline
\frac {\partial a}{\partial c} = 1
\newline
\to \frac {\partial J}{\partial c} 
= \frac {\partial J}{\partial d} \frac {\partial d}{\partial a} \frac {\partial a}{\partial c}
= b = 2
\end{array}
$$
æœ€åï¼Œæ¨å¯¼$\frac {\partial J}{\partial w}$
$$
\begin{array}{l}
\frac {\partial c}{\partial w} = x
\newline\to
\frac {\partial J}{\partial a} 
= \frac {\partial J}{\partial d} \frac {\partial d}{\partial a} \frac {\partial a}{\partial c} \frac {\partial c}{\partial w}
= d * x = -4
\end{array}
$$


![image-20230719175805121](D:\CS\Machine Learning\Course_2_Advanced Algorithm\4-Training a Neural Network in TensorFlow.assets\image-20230719175805121.png)



ä½¿ç”¨åå‘ä¼ æ’­è®¡ç®—å¯¼æ•°æœ€å¤§çš„ä¼˜åŠ¿åœ¨äºèƒ½å¤Ÿé‡å¤åˆ©ç”¨å·²ç»è®¡ç®—è¿‡çš„å¯¼æ•°å€¼ï¼ˆä¾‹å¦‚ä¸Šé¢çš„$\frac {\partial J}{\partial a}$ï¼‰ï¼Œä½¿å¾—è®¡ç®—æ›´åŠ å¿«é€Ÿã€‚

å¯¹äºå…·æœ‰Nä¸ªèŠ‚ç‚¹ï¼ŒPä¸ªå‚æ•°çš„è®¡ç®—å›¾ï¼Œå‰å‘è®¡ç®—å¯¼æ•°çš„æ—¶é—´å¤æ‚åº¦ä¸º`O(N * P)`ï¼Œè€Œåå‘è®¡ç®—å¯¼æ•°çš„æ—¶é—´å¤æ‚åº¦ä¸º`O(N + P)`ã€‚



## Larger Network

ç»™å‡ºä¸€ä¸ªä¸¤å±‚ï¼Œæ¯å±‚ä¸€ä¸ªç¥ç»å…ƒçš„ç¥ç»ç½‘ç»œï¼Œè¿›ä¸€æ­¥æ·±å…¥ç†è§£è®¡ç®—å›¾å’Œåå‘ä¼ æ’­

é¦–å…ˆæ ¹æ®å‰å‘ä¼ æ’­ç»˜åˆ¶å‡ºè®¡ç®—å›¾

![image-20230719182254952](D:\CS\Machine Learning\Course_2_Advanced Algorithm\4-Training a Neural Network in TensorFlow.assets\image-20230719182254952.png)

å†™å‡ºæ¯éƒ¨åˆ†çš„å¯¼æ•°
$$
\begin{array}{c}
\begin{array}{l}
\frac {\partial z_1}{\partial b_1} = 1\\
\frac {\partial z_1}{\partial w_1} = x
\end{array} &
\begin{array}{l}
\frac {\partial z_2}{\partial b_2} = 1\\
\frac {\partial z_2}{\partial w_2} = z_1\\
\frac {\partial z_2}{\partial z_1} = w_2
\end{array} &
\begin{array}{l}
\frac {\partial J}{\partial z_2} = z_2 - y
\end{array}
\end{array}
$$
é“¾å¼æ±‚å¯¼
$$
\begin{array}{l}
\frac {\partial J}{\partial b_2} = z_2 - y\\
\frac {\partial J}{\partial w_2} = z_1(z_2 - y)\\
\frac {\partial J}{\partial b_1} = w_2(z_2 - y)\\
\frac {\partial J}{\partial w_1} = w_2(z_2 - y)x
\end{array}
$$



## Accumulated error BP

ä¸Šè¿°åå‘ä¼ æ’­çš„æ–¹æ³•å¯ä»¥ç”¨ä¸‹é¢çš„ä¼ªç è¡¨ç¤º
$$
\underline{
\over {
\begin{array}{l}
è¾“å…¥:è®­ç»ƒé›†D = \{(x_i, y_i)\}_{i = 1}^m;\\
\quad\quad\quadå­¦ä¹ ç‡\alpha.\\
è¿‡ç¨‹:\\
1. éšæœºåˆå§‹åŒ–æƒé‡ï¼Œå€¼åŸŸ(0, 1)\\
2. \mathbf{repeat}\\
\quad\quad \mathbf{for\ all\ (x_k, y_k)\ \in D\ do}\\
\quad\quad\quad è®¡ç®—æ¢¯åº¦;\\
\quad\quad\quad æ›´æ–°æƒé‡;\\
\quad\quad \mathbf{end\ for}\\
3. \mathbf{until}\ åœæ­¢æ¡ä»¶\\
è¾“å‡º: ç¡®å®šæƒé‡çš„ç¥ç»ç½‘ç»œ
\end{array}
}
}
$$


è¯¥ç®—æ³•ä¸ºâ€æ ‡å‡†BPç®—æ³•"ï¼Œæ¯æ¬¡åªé’ˆå¯¹ä¸€ä¸ªæ ·ä¾‹æ›´æ–°æƒé‡ï¼Œè€ŒBPç®—æ³•çš„ç›®æ ‡æ˜¯**æœ€å°åŒ–**æ•´ä¸ªè®­ç»ƒé›†$D$ä¸Šçš„**ç´¯è®¡è¯¯å·®**ï¼š
$$
E = \frac 1 m \sum_{i = 1}^m E_i
$$


å› æ­¤å¯ä»¥åœ¨è¯»å–æ•´ä¸ªè®­ç»ƒé›†$D$ä¹‹åï¼Œæ‰å¯¹å‚æ•°è¿›è¡Œæ›´æ–°ï¼Œè¿™ç§æ–¹æ³•ç§°ä¸ºAccumulated BPï¼ˆç´¯è®¡è¯¯å·®é€†ä¼ æ’­ï¼‰ã€‚ç›¸æ¯”äºæ ‡å‡†BPåœ¨è¯»å–æ¯ä¸ªæ ·ä¾‹ä¹‹åéƒ½ä¼šæ›´æ–°æƒé‡ï¼Œç´¯è®¡BPæ›´æ–°é¢‘ç‡æ›´ä½ï¼Œä¸”å› ä¸ºç›´æ¥æ›´æ–°ç´¯è®¡è¯¯å·®ï¼Œé€šå¸¸æ¯”æ ‡å‡†BPéœ€è¦çš„è¿­ä»£æ¬¡æ•°æ›´å°‘ï¼›ç„¶è€Œç´¯è®¡BPä¹Ÿå­˜åœ¨è¯¯å·®ä¸‹é™åˆ°ä¸€å®šç¨‹åº¦åï¼Œä¸‹é™å˜ç¼“çš„é—®é¢˜ï¼Œå› æ­¤åœ¨$D$éå¸¸å¤§çš„æ—¶å€™ï¼Œé€‰æ‹©æ ‡å‡†BPèƒ½å¤Ÿæ›´å¿«è·å¾—è¾ƒå¥½çš„è§£ã€‚



# Overfitting

ç”±äºç¥ç»ç½‘ç»œçš„è¡¨ç°èƒ½åŠ›éå¸¸å¼ºï¼Œå¾ˆå®¹æ˜“å‡ºç°è¿‡æ‹Ÿåˆï¼Œè§£å†³ç¥ç»ç½‘ç»œè¿‡æ‹Ÿåˆé€šå¸¸æœ‰ä¸¤ç§ç­–ç•¥ï¼š

- early stoppingã€‚å°†æ•°æ®åˆ’åˆ†ä¸ºè®­ç»ƒé›†å’ŒéªŒè¯é›†ï¼Œä½¿ç”¨éªŒè¯é›†åœ¨è®­ç»ƒçš„æ—¶å€™ä¼°è®¡è¯¯å·®ï¼Œè‹¥è®­ç»ƒé›†è¯¯å·®é™ä½ï¼ŒéªŒè¯é›†è¯¯å·®å‡é«˜ï¼Œåˆ™åœæ­¢è®­ç»ƒï¼ŒéªŒè¯é›†è¯¯å·®æœ€å°çš„æƒé‡
- regularizationã€‚åœ¨costä¸­å¼•å…¥æ­£åˆ™é¡¹ï¼Œæ­£å¦‚å‰é¢ç¬”è®°ä¸­è®¨è®ºçš„ä¸€æ ·ã€‚



# Local & Global Minimum

costå‡½æ•°å¯èƒ½å­˜åœ¨å¤šä¸ªå±€éƒ¨æœ€å°å€¼ï¼Œæˆ‘ä»¬é€šå¸¸å¸Œæœ›æœ€ä¼˜åŒ–ç®—æ³•èƒ½å¤Ÿå¯»æ‰¾åˆ°å…¨å±€çš„æœ€å°å€¼ã€‚ç„¶è€ŒåŸºäºæ¢¯åº¦çš„ä¼˜åŒ–æ–¹æ³•ï¼Œåœ¨costå‡½æ•°å­˜åœ¨å¤šä¸ªå±€éƒ¨æœ€å°çš„æ—¶å€™ï¼Œæ— æ³•ä¿è¯ä¸€å®šèƒ½æ‰¾åˆ°å…¨å±€æœ€å°ã€‚é€šå¸¸æœ‰ä»¥ä¸‹ç­–ç•¥ç”¨æ¥è§£å†³è¿™ç§é—®é¢˜ï¼š

- ç”¨å¤šç»„åˆå§‹åŒ–å‚æ•°è®­ç»ƒå¤šä¸ªç½‘ç»œï¼Œé€‰æ‹©æœ€ä¼˜è§£
- æ¨¡æ‹Ÿé€€ç«ï¼ˆsimulated annealingï¼‰ï¼Œä»¥ä¸€å®šæ¦‚ç‡æ¥å—æ›´å·®çš„ç»“æœï¼Œâ€è·³å‡ºâ€œå±€éƒ¨æœ€å°ï¼›æ¥å—â€æ¬¡ä¼˜è§£â€œçš„æ¦‚ç‡éšè¿­ä»£æ•°å‡é«˜è€Œé™ä½
- éšæœºæ¢¯åº¦ä¸‹é™ï¼ˆSGDï¼‰ã€‚
- é—ä¼ ç®—æ³•ï¼ˆgenetic algorithmsï¼‰ã€‚

ä¸è¿‡è¿™äº›ç®—æ³•éƒ½æ˜¯å¯å‘å¼çš„ï¼Œç†è®ºä¸Šç¼ºä¹ä¿éšœã€‚


# å‚è€ƒ

- å´æ©è¾¾ã€Šæœºå™¨å­¦ä¹ 2022ã€‹
- è¥¿ç“œä¹¦
- [Deep Learning From Scratch: Theory and Implementation](https://www.codingame.com/playgrounds/9487/deep-learning-from-scratch---theory-and-implementation/computational-graphs)
- [Rectified Linear Units](https://deepai.org/machine-learning-glossary-and-terms/rectified-linear-units)
- [ä¸€æ–‡ææ‡‚ç†µ(Entropy),äº¤å‰ç†µ(Cross-Entropy)](https://zhuanlan.zhihu.com/p/149186719)
