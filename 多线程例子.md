# Motivation

在首届世界科学智能大赛-生科赛道中，数据集特征维度非常大（483512），在预筛选特征的过程中，尝试使用MI来作为筛选依据，计算MI对CPU的占用不大，但sklearn的相关算法没有并发处理，效率非常差。考虑进行并行处理

基本的并行框架

```py
from multiprocessing import Pool
# 创建进程池Pool，例如4进程
p = Pool(4)
# 将任务加入进程池
for i in range():
    res.append(p.apply_async(function, args=(,)).get()) # function为需要并行的函数，args为函数参数列表
    												# .get()用于获取function的返回值
p.close()
p.join() # 先close()再join()
```



改写前的互信计算框架

```py
def ML_filter(chunk, target):
    from sklearn.feature_selection import mutual_info_regression
    '''
    Args:
    chunk (m,n): DataFrame of m examples, n features
    target (m,): m target value
    thresh (scalar): thresh of pearson filter
    
    Return:
    relation (n,): array of ml
    '''
    ml = mutual_info_regression(chunk, target)
    return ml
    
def ML_selection(train_dir, map_dir, batch_size = 5000):
    '''
    read h5 file then run pearson_filter in turns
    Args:
    train_dir (str): dir of h5 file
    map_dir (str): dir of map file
    batch_size (scalar): size per batch
    
    Return:
    relation (DataFrame): bool values of each cpgsite
    '''
    age = pd.read_csv('../ai4bio/trainmap.csv',
                  sep=',',
                  index_col = 0)['age']
    relation = []
    with h5py.File(train_dir) as file:
        relation = np.zeros((file['data'].shape[1]))
        for index in range(0,file['data'].shape[1],batch_size): # 分组读取数据
            print(f'Processing {index}', end ='\r')
            data=file['data'][:,index:index+batch_size]
            out = ML_filter(data, age)
            relation[index:index+batch_size] = out
    return relation        
```



改写后

```py
def multiprocessing_mi(data_dir, target_dir, batch_size, workers = 4):
    from multiprocessing import Pool
    p = Pool(workers)
    target = pd.read_csv(target_dir,sep=',',index_col = 0)['age']

    with h5py.File(data_dir, 'r') as file:
        relation = np.zeros((file['data'].shape[1]))
        for index in range(0,file['data'].shape[1],batch_size): # 分组读取数据
            print(f'Processing {index}', end ='\r')
            data=file['data'][:,index:index+batch_size]
            relation[index:index+batch_size] = p.apply_async(mutual_info_regression, args=(data, target)).get() 
            # .get()用于获取返回值
        p.close()
        p.join()
    return relation
relation = multiprocessing_mi(train_dir, map_dir, batch_size = 5, workers = 8)
```

